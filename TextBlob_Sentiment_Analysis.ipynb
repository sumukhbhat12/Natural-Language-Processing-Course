{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyMuCOVuAg0palxwZmFX8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumukhbhat12/Natural-Language-Processing-Course/blob/main/TextBlob_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset used is imdb dataset sentiment analysis in csv format**\n",
        "https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format"
      ],
      "metadata": {
        "id": "THJZ_VCH93T-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d_RPFnCv89jk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the data**\n",
        "\n",
        "remove the < br> < /br> from the dataset to remove the tokenizing parseError"
      ],
      "metadata": {
        "id": "2bE8p_so_KkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/Train.csv')\n",
        "\n",
        "print(train)"
      ],
      "metadata": {
        "id": "sbtNq9-P_NPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdb4d81-a5cf-4659-b66a-181a1f155936"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0      I grew up (b. 1965) watching and loving the Th...      0\n",
            "1      When I put this movie in my DVD player, and sa...      0\n",
            "2      Why do people who do not know what a particula...      0\n",
            "3      Even though I have great interest in Biblical ...      0\n",
            "4      Im a die hard Dads Army fan and nothing will e...      1\n",
            "...                                                  ...    ...\n",
            "39995  \"Western Union\" is something of a forgotten cl...      1\n",
            "39996  This movie is an incredible piece of work. It ...      1\n",
            "39997  My wife and I watched this movie because we pl...      0\n",
            "39998  When I first watched Flatliners, I was amazed....      1\n",
            "39999  Why would this film be so good, but only gross...      1\n",
            "\n",
            "[40000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "divide the dataset into 5k samples of label-0's and label-1's respectively and then concat them into a new training dataset\n",
        "This is to reduce the size of the dataset while maintaining equal samples of both 0's and 1's labels "
      ],
      "metadata": {
        "id": "qfrUjZx7HkwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_0 = train[train['label'] == 0].sample(n=5000)\n",
        "label_1 = train[train['label'] == 1].sample(n=5000)\n",
        "\n",
        "train = pd.concat([label_1,label_0])"
      ],
      "metadata": {
        "id": "5D_h3UUwHJWM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the train dataset"
      ],
      "metadata": {
        "id": "Jcq8WYtjH4XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "train = shuffle(train)"
      ],
      "metadata": {
        "id": "xRZq4NbdH66H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "hS6Ke3wUIHVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WkmKaO7IJ4_",
        "outputId": "2228daad-0a10-49f2-d1e7-71ccf7ffd700"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace \"only space\" characters in the string with Nan and replace tab, newline and carriage return escape sequences with empty string"
      ],
      "metadata": {
        "id": "RJSR3DAqvMvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
        "\n",
        "train.replace(to_replace=[r'\\\\t|\\\\n|\\\\r', '\\t|\\n|\\r'], value=['',''], regex=True, inplace=True)\n",
        "\n",
        "train.dropna(axis=0, how='any', inplace=True)"
      ],
      "metadata": {
        "id": "GxybG0nGIRlW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter non ascii text and ignore them\n",
        "\n",
        "encode the string into ascii encoding and ignore the non convertibles, then decode the asii encoded string back"
      ],
      "metadata": {
        "id": "ftpf6Ko3v_zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].str.encode('ascii','ignore').str.decode('ascii')"
      ],
      "metadata": {
        "id": "CtmkeEynvi-Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "punctuations !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~"
      ],
      "metadata": {
        "id": "bpJ3z6IMyXRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "  import string\n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation,'')\n",
        "  return text\n",
        "\n",
        "train['text'] = train['text'].apply(remove_punctuations)"
      ],
      "metadata": {
        "id": "-tn5DeunxdTa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "QL8nXJE7yagm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "don't keep not and no in the stopword list because we need no and not words for sentiment analysis"
      ],
      "metadata": {
        "id": "5dwzL_V6zP28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "metadata": {
        "id": "OH8en26VyxUb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ToktokTokenizer()"
      ],
      "metadata": {
        "id": "rmRTOwyPzybz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_remove_stopwords(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "I53jbf_TzdhU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(custom_remove_stopwords)"
      ],
      "metadata": {
        "id": "YXfcr-060sD2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "  text = re.sub('[^a-zA-Z0-9\\s]','',text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "A0dLauLX1I6F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "3LqctTsU1bYL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "  import re\n",
        "  html_pattern = re.compile('<.*?>')\n",
        "  return html_pattern.sub(r' ', text)"
      ],
      "metadata": {
        "id": "6XjMKgk-1lwJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(remove_html)"
      ],
      "metadata": {
        "id": "fVqlNgLY2DHX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\S matches anything not matched by \\s"
      ],
      "metadata": {
        "id": "1pN-mOjd3oDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return url.sub(r' ', text)"
      ],
      "metadata": {
        "id": "Q3xYzvtV2Kj-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(remove_url)"
      ],
      "metadata": {
        "id": "xhjCFfDg3yJj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  text = ''.join([i for i in text if not i.isdigit()])\n",
        "  return text\n",
        "\n",
        "train['text'] = train['text'].apply(remove_numbers)"
      ],
      "metadata": {
        "id": "4ewcMVfK36kx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove everything that ends with a digit"
      ],
      "metadata": {
        "id": "V96IboM-5BBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanse(word):\n",
        "  rx = re.compile(r'\\D*\\d')\n",
        "\n",
        "  if rx.match(word):\n",
        "    return ''\n",
        "  return word\n",
        "\n",
        "def remove_alphanumeric(strings):\n",
        "  nstrings = [\" \".join(filter(None, (cleanse(word) for word in string.split()))) for string in strings.split()]\n",
        "  str1 = ' '.join(nstrings)\n",
        "  return str1"
      ],
      "metadata": {
        "id": "BU-XweYk4pve"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(remove_alphanumeric)"
      ],
      "metadata": {
        "id": "xcOPhElE7w3B"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "EBQxdiVY75RR",
        "outputId": "bb824fd3-1477-463d-f8bc-27e977cf796d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "33387  film almost complete waste time studying book ...      0\n",
              "36849  kidding weight loss thing well might lose weig...      1\n",
              "33456  nineteen eighty two announced Dismisal going m...      1\n",
              "30515  fabulous filmwhich watched several times since...      1\n",
              "31483  Chesty gringo Telly Savalas Frank Cooper USMex...      0\n",
              "...                                                  ...    ...\n",
              "7792   really cant remember recommended said one favo...      1\n",
              "37649  Tooth Fairy ghost old deformed witch lures chi...      0\n",
              "14795  Oh well thought good action not Although Jeff ...      0\n",
              "11818  Treading Water beautiful movie would put strai...      1\n",
              "1644   trailer deceiving thought good film point brin...      0\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efc4503a-deb9-4317-a291-01dbfe7edfb5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33387</th>\n",
              "      <td>film almost complete waste time studying book ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36849</th>\n",
              "      <td>kidding weight loss thing well might lose weig...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33456</th>\n",
              "      <td>nineteen eighty two announced Dismisal going m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30515</th>\n",
              "      <td>fabulous filmwhich watched several times since...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31483</th>\n",
              "      <td>Chesty gringo Telly Savalas Frank Cooper USMex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7792</th>\n",
              "      <td>really cant remember recommended said one favo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37649</th>\n",
              "      <td>Tooth Fairy ghost old deformed witch lures chi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14795</th>\n",
              "      <td>Oh well thought good action not Although Jeff ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>Treading Water beautiful movie would put strai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>trailer deceiving thought good film point brin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efc4503a-deb9-4317-a291-01dbfe7edfb5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efc4503a-deb9-4317-a291-01dbfe7edfb5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efc4503a-deb9-4317-a291-01dbfe7edfb5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatize the text**"
      ],
      "metadata": {
        "id": "pT0k-Q4u9Oj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "# rand_text = nlp('Hello, my name is Sumukh, Nice meeting you. How are you doing, I am doing quite well, what about you? what are you doing currently? I am currently at a party')\n",
        "# for word in rand_text:\n",
        "  # print(word.lemma_)"
      ],
      "metadata": {
        "id": "JBYeKAvZ9jUs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy has a special lemma : -PRON-. This is used as the lemma for all pronouns such as Their , you , me , and I"
      ],
      "metadata": {
        "id": "rC3MncncBsEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "  text = nlp(text)\n",
        "  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "Pfi51BTX84u6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "Ti_qo-6lCDFN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "SpnBMTWZCcuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['sentiment'] = train['text'].apply(lambda tweet: TextBlob(tweet).sentiment)"
      ],
      "metadata": {
        "id": "1pjNkqRuCgK5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "Polarity is the output that lies between [-1,1], where -1 refers to negative sentiment and +1 refers to positive sentiment. Subjectivity is the output that lies within [0,1] and refers to personal opinions and judgments."
      ],
      "metadata": {
        "id": "PhI56glmDXEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_series = train['sentiment'].tolist()\n",
        "\n",
        "# print(sentiment_series)\n",
        "\n",
        "# sentiment column is a tuple with values like (-0.353625, 0.783234) etc.\n",
        "\n",
        "columns = ['polarity', 'subjectivity']\n",
        "\n",
        "# we separate the sentiment column and make a new dataframe and split the column into polarity and subjectivity\n",
        "df1 = pd.DataFrame(sentiment_series, columns=columns, index=train.index)\n",
        "\n",
        "# print(df1.head(10))\n",
        "\n",
        "#we concatenate the df1 and train dataframe to get sentiment, polarity and subjectivity columns all together\n",
        "result = pd.concat([train, df1], axis=1)\n",
        "\n",
        "# print(result.head(10))\n",
        "\n",
        "#we no longer need sentiment column in the final result, hence we drop it\n",
        "result.drop(['sentiment'], axis=1, inplace=True)\n",
        "\n",
        "result.loc[result['polarity'] >= 0.2, 'Sentiment'] = 'Positive'\n",
        "result.loc[result['polarity'] < 0.2, 'Sentiment'] = 'Negative'\n",
        "\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdoQk9s2DGg3",
        "outputId": "ed3f64e5-620e-4fc9-e00d-b3a6a53b7fa8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label  polarity  \\\n",
            "33387  film almost complete waste time study book eng...      0 -0.096259   \n",
            "36849  kid weight loss thing well might lose weight n...      1  0.285714   \n",
            "33456  nineteen eighty two announce Dismisal going ma...      1  0.245455   \n",
            "30515  fabulous filmwhich watch several time since bu...      1  0.230003   \n",
            "31483  chesty gringo Telly Savalas Frank Cooper USMex...      0  0.065556   \n",
            "...                                                  ...    ...       ...   \n",
            "7792   really can not remember recommend say one favo...      1  0.049669   \n",
            "37649  Tooth Fairy ghost old deform witch lure child ...      0  0.119801   \n",
            "14795  oh well think good action not although Jeff Sp...      0  0.183333   \n",
            "11818  tread Water beautiful movie would put straight...      1  0.375000   \n",
            "1644   trailer deceive think good film point bring wo...      0 -0.024074   \n",
            "\n",
            "       subjectivity Sentiment  \n",
            "33387      0.354082  Negative  \n",
            "36849      0.465306  Positive  \n",
            "33456      0.390909  Positive  \n",
            "30515      0.639986  Positive  \n",
            "31483      0.485000  Negative  \n",
            "...             ...       ...  \n",
            "7792       0.455565  Negative  \n",
            "37649      0.444587  Negative  \n",
            "14795      0.463889  Negative  \n",
            "11818      0.575992  Positive  \n",
            "1644       0.603704  Negative  \n",
            "\n",
            "[10000 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ]
}